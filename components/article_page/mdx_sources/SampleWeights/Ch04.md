<table style="width:50%">
<tr><th style="width:50%; text-align: center">Julia</th>
<th style="width:50%; text-align: center">Python</th></tr><tr>
<td style="border: 1px solid transparent">

```Julia
function f(
    x::Nothing,
    y::Nothing,
    z::Nothing
)::Nothing
```
</td><td style="border: 1px solid transparent">

```python
def f(
    x:None,
    y:None,
    z: None
) -> None:
```
</td></tr><tr><td colspan="2" style="text-align: center">
        View More: <a href="https://www.github.com/risklabai/RiskLabAI.jl">Julia</a> | <a href="https://www.github.com/risklabai/RiskLabAI.jl">Python</a>
</td></tr></table>


# <span style="color:cyan">Sample Weights</span>

## How many models have you seen in finance that do not make the <span style="color:cyan">IID assumption</span>? Probably not as much!

Here you will learn how to use sample weights to address financial applications' problems: observations usually need to follow independent and identically distributed processes. 

## Where and how does the <span style="color:cyan">IID assumption fail</span> in finance?

Since financial labels might be based on overlapping intervals, they are not independent and identically distributed. Most machine learning applications require IID assumption, which is sometimes true but primarily false in real-world financial applications. Here, we introduce some methods to tackle this challenge.

## How can we define <span style="color:cyan">concurrency</span> for financial labels?

We call two labels $y_{i}$ and $y_{j}$ concurrent when they depend on a unique return.

We define an indicator function $\mathbb{I}_{t, i}$ that is 1 if and only if $\left[t_{i, 0}, t_{i, 1}\right]$  overlaps with $[t-1, t]$  and it is zero otherwise.
Therefore, the number of labels concurrent at time $t$ is represented by $c_{t}=\sum_{i=1}^{I} \mathbb{I}_{t, i}$. 

In the RiskLabAI's Julia library, the concurrency is calculated using the `concurrencyEvents` function. This function takes three inputs: 
* `closeIndex` (which is the data frame of close prices)
* `timestamp` (which is a data frame that has both returns and labels)
* `molecule` (the indices used when multithreading is applied). 
  
Similarly, in RiskLabAI's python library, the function TBD does the job.

<table style="width:90%">
<tr><th style="width:50%; text-align: center">Julia</th>
<th style="width:50%; text-align: center">Python</th></tr><tr>
<td style="border: 1px solid transparent">

```julia 
function concurrencyEvents(
      closeIndex ::DataFrame, 
      timestamp ::DataFrame,
      molecule ::Vector 
)::DataFrame
```
</td><td style="border: 1px solid transparent">

```python
def concurrencyEvents(
      closeIndex:pd.DataFrame, 
      timestamp:pd.DataFrame, 
      molecule: pd.Series
) -> pd.DataFrame
```
</td></tr><tr><td colspan="2" style="text-align: center">
        View More: <a href="https://www.github.com/risklabai/RiskLabAI.jl">Julia</a> | <a href="https://www.github.com/risklabai/RiskLabAI.jl">Python</a>
</td></tr></table>



## Now that we know how to define a concurrency measure for labels, let us use it to define <span style="color:cyan">label uniqueness</span>!

We first define a function $u_{t, i}=\frac{{\mathbb I}_{t, i}}{c_{t}}$ that show uniqueness of a label $i$ at time $t$ .
Second, we define the average uniqueness of label $i$ as below:

$$
\bar{u}_{i}= \frac{\sum_{t=1}^{T} u_{t, i}}{\sum_{t=1}^{T} {\mathbb I}_{t, i}}
$$

This figure plots the histogram of uniqueness values derived from an object $t_1$. 

<figure><img src="Figs\averageuniqness.png" width="50%" alt="figure could not be uploaded..."/>
<figcaption><span style="color:DimGray; font-weight:bold">Histogram of uniqueness values</span></figcaption></figure>

In RiskLabAI's Julia library label uniqueness is estimated using the `sampleWeight` function. This function takes three inputs:

* `timestamp` (the data frame of start and end dates)
* `concurrencyEvents` (data frame of concurrent events generated by the function `concurrencyEvents`)
* `molecule` (which determines the index used in multithreading)
  
Similarly, in RiskLabAI's python library, the function `mpSampleWeight` does the job. 

<table style="width:90%">
<tr><th style="width:50%; text-align: center">Julia</th>
<th style="width:50%; text-align: center">Python</th></tr><tr>
<td style="border: 1px solid transparent">

```julia
function sampleWeight(
      timestamp ::DataFrame, 
      concurrencyEvents ::DataFrame, 
      molecule ::Vector
)::DataFrame
```
</td><td style="border: 1px solid transparent">

```python
def mpSampleWeight(
      timestamp :pd.DataFrame,
      concurrencyEvents :pd.DataFrame,
      molecule :pd.Series
) -> None
```
</td></tr><tr><td colspan="2" style="text-align: center">
        View More: <a href="https://www.github.com/risklabai/RiskLabAI.jl">Julia</a> | <a href="https://www.github.com/risklabai/RiskLabAI.jl">Python</a>
</td></tr></table>



## Bootstrapping fails when we have <span style="color:cyan">overlapping outcomes</span>!

We want to select $I$ items from a set of $I$ items with replacements. The probability of not selecting one particular element is $\left(1-I^{-1}\right)^{I}$ so as the set size grows this probability converge to $e^{-1}$ and this means that expected number of unique observation is $\left(1-e^{-1}\right) \approx \frac{2}{3}$.

The situation worsens when the number of non-overlapping outcomes is less than $I$. In this case, uniqueness becomes less than $1-e^{-1}$. Of course, bootstrapping becomes inefficient when the number of overlapping outcomes is higher. The most obvious method is to drop overlapping outcomes before performing the bootstrap! Since overlapping is partial, this technique serves like a double-edged sword: deleting overlapping outcomes results in losing valuable information! 

## Let us see how <span style="color:cyan"> Sequential Bootstrapping </span> can solve the overlapping outcomes problem!

We can solve the overlapping outcomes problem by sampling observations with different probabilities.

Consider the probability density for sampling the observation. We show set of selected observation until step $m$ with $\mathcal{S}_m$ and we define the probability density that we select observation $i$ in step $m$ by this equation:

$$
\delta_{i}^{(m)}=\bar{u}_{i}^{(m)}\left(\sum_{k=1}^{I} \bar{u}_{k}^{(m)}\right)^{-1}
$$

where

$$
\bar{u}_{i}^{(m)} = \frac{\sum_{t=1}^{T} u_{t, i}^{(m)}}{\sum_{t=1}^{T} {\mathbb{I}}_{t, i}}
$$

and

$$
u_{t, i}^{(m)}={\mathbb I}_{t, i}\left(1+\sum_{k \in \mathcal{S}_m} {\mathbb{I}}_{t, k}\right)^{-1}
$$

At the first step, we choose $\delta_{i}^{(1)}=I^{-1}$. Sequential bootstrap sampling will be much closer to IID than the standard bootstrap method because it assigns a smaller probability to overlapping outcomes.

Now let us move on to the implementation of the sequential bootstrap method.

In the RiskLabAI's Julia library, the index matrix is calculated using the `indexMatrix` function. This function takes two inputs:
* `barIndex` (a data frame index of input data)
* `timestamp` (a data frame with both returns and labels).

Similarly, in RiskLabAI's python library, the function `index_matrix` does the job.

<table style="width:80%">
<tr><th style="width:50%; text-align: center">Julia</th>
<th style="width:50%; text-align: center">Python</th></tr><tr>
<td style="border: 1px solid transparent">

```Julia
function indexMatrix(
      barIndex ::Vector,
      timestamp ::DataFrame
)::Matrix
```
</td><td style="border: 1px solid transparent">

```python
def f(
      x:None,
      y:None,
      z: None
) -> None:
```
</td></tr><tr><td colspan="2" style="text-align: center">
        View More: <a href="https://www.github.com/risklabai/RiskLabAI.jl">Julia</a> | <a href="https://www.github.com/risklabai/RiskLabAI.jl">Python</a>
</td></tr></table>

In the RiskLabAI's Julia library, the average uniqueness is calculated using the `averageUniqueness` function. This function takes one input: 
* `IndexMatrix` (a matrix that calculates by `indexMatrix` function). 

Similarly, in RiskLabAI's python library, the function `averageUniqueness` does the job.

<table style="width:80%">
<tr><th style="width:50%; text-align: center">Julia</th>
<th style="width:50%; text-align: center">Python</th></tr><tr>
<td style="border: 1px solid transparent">

```Julia
function averageUniqueness(
      IndexMatrix ::Matrix
)::Vector
```
</td><td style="border: 1px solid transparent">

```python
def averageUniqueness(
      indexMatrix
) -> None:
```
</td></tr><tr><td colspan="2" style="text-align: center">
        View More: <a href="https://www.github.com/risklabai/RiskLabAI.jl">Julia</a> | <a href="https://www.github.com/risklabai/RiskLabAI.jl">Python</a>
</td></tr></table>

In the RiskLabAI's Julia library, we sample with the sequential bootstrap method using the `sequentialBootstrap` function. This function takes two inputs: 
* `indexMatrix `(the matrix calculated by the `indexMatrix` function). 
* `sampleLength` (the number of samples)
  
Similarly, in RiskLabAI's python library, the function `SequentialBootstrap` does the job.

<table style="width:80%">
<tr><th style="width:50%; text-align: center">Julia</th>
<th style="width:50%; text-align: center">Python</th></tr><tr>
<td style="border: 1px solid transparent">

```Julia
function sequentialBootstrap(
    indexMatrix ::Matrix,  
    sampleLength ::Int64  
)
```
</td><td style="border: 1px solid transparent">

```python
def sequential_bootstrap(indexMatrix :np.array, 
                        sampleLength :int):
```
</td></tr><tr><td colspan="2" style="text-align: center">
        View More: <a href="https://www.github.com/risklabai/RiskLabAI.jl">Julia</a> | <a href="https://www.github.com/risklabai/RiskLabAI.jl">Python</a>
</td></tr></table>

## A <span style="color:cyan">Monte Carlo</span> experiment can now verify our method's effectiveness!

We want to evaluate our method. For this purpose, we generate random timestamps. This function gets three inputs: the number of observations, the number of bars, and the maximum holding period.
At each observation, we generate a random number less than the number of starting time bars and another random number less than maximumHolding for the ending time.

In the RiskLabAI's Julia library, we generate a random timestamp by `randomTimestamp` function. This function takes three inputs: 
* `nObservations` (a matrix calculated by indexMatrix function)
* `nBars` (number of bars)
* `maximumHolding` (maximum holding period)
  
Similarly, in RiskLabAI's python library, the function `randomTimestamp` does the job.

<table style="width:80%">
<tr><th style="width:50%; text-align: center">Julia</th>
<th style="width:50%; text-align: center">Python</th></tr><tr>
<td style="border: 1px solid transparent">

```julia
function randomTimestamp(nObservation ::Int64,
                         nBars ::Int64, 
                         maximumHolding
                        )::DataFrame
end
```
</td><td style="border: 1px solid transparent">

```python
def randomTimestamp(nObservation -> int,
                    nBars -> int,
                    maximumHolding)-> None :
```
</td></tr><tr><td colspan="2" style="text-align: center">
        View More: <a href="https://www.github.com/risklabai/RiskLabAI.jl">Julia</a> | <a href="https://www.github.com/risklabai/RiskLabAI.jl">Python</a>
</td></tr></table>

In the RiskLabAI's Julia library, we implement monte carlo simulation with sequentional bootstrap and compare it with standard bootstrap with `monteCarloSimulationforSequentionalBootstraps` function. This function takes three inputs: 
* `nObservation` (number of observations)
* `nBars` (number of bars)
* `maximumHolding` (maximum holding period)
  
Similarly, in RiskLabAI's python library, the function `MonteCarloSimulationforSequentionalBootstraps` does the job.

<table style="width:80%">
<tr><th style="width:50%; text-align: center">Julia</th>
<th style="width:50%; text-align: center">Python</th></tr><tr>
<td style="border: 1px solid transparent">

```julia 
function 
monteCarloSimulationforSequentionalBootstraps(
      nObservation ::Int64,
      nBars ::Int64, 
      maximumHolding::Int64
)::Tuple{Float64,Float64}
end
```
</td><td style="border: 1px solid transparent">

```python
def MonteCarloSimulationforSequentionalBootstraps(
      nObservation:int,
      nBars :int,
      maximumHolding:int) ->None:
```
</td></tr><tr><td colspan="2" style="text-align: center">
        View More: <a href="https://www.github.com/risklabai/RiskLabAI.jl">Julia</a> | <a href="https://www.github.com/risklabai/RiskLabAI.jl">Python</a>
</td></tr></table>

In the RiskLabAI's Julia library, we  run `monteCarloSimulationforSequentionalBootstraps`
in multiple iteration and also do this job for standard bootstrap and save their with `SimulateSequentionalVsStandardBootstrap`.

this function takes four inputs: 
* `iteration` (number of iteration)
* `nObservation` (number of observations)
* `nBars` (number of bars)
* `maximumHolding` (maximum holding period)
  
Similarly, in RiskLabAI's python library, the function `SimulateSequentionalVsStandardBootstrap` does the job.


<table style="width:80%">
<tr><th style="width:50%; text-align: center">Julia</th>
<th style="width:50%; text-align: center">Python</th></tr><tr>
<td style="border: 1px solid transparent">

```julia 
function 
SimulateSequentionalVsStandardBootstrap(
      iteration::Int64,    
      nObservation::Int64,
      nBars::Int64,
      maximumHolding::Int64
)::Tuple{Vector,Vector}
```
</td><td style="border: 1px solid transparent">

```python
def SimulateSequentionalVsStandardBootstrap(
      iteration :int,
      nObservation :int,
      nBars :int,
      maximumHolding :int) -> None:
```
</td></tr><tr><td colspan="2" style="text-align: center">
        View More: <a href="https://www.github.com/risklabai/RiskLabAI.jl">Julia</a> | <a href="https://www.github.com/risklabai/RiskLabAI.jl">Python</a>
</td></tr></table>

This figure shows the result of a monte carlo test to compare the performance of standard and sequential bootstraps. As you can seen ....[the description]

<figure><img src="Figs\seqvsstan.png" width="50%" alt="put alternative text here..."/>
<figcaption><span style="color:DimGray; font-weight:bold">Standard versus Sequential Bootstrap</span></figcaption></figure>



In figure $4.2$ we see histogram of average uniqueness of standard bootstrapped samples (left) and the sequentially bootstrapped samples (right).

## Same weight for every return?!
In the last section, we learned how to bring bootstrap samples closer to IID. This Section describes a method for weighing such data in order to train a machine learning system. The weights of substantially overlapping outcomes would be excessive if they were regarded comparable to non-overlapping outcomes. Labels with high absolute returns should be given more weight than those with low absolute returns. We must evaluate observations based on their uniqueness as well as their absolute return.




In the RiskLabAI's Julia library, the sample weight with return attribution is calculated using the `sampleWeight` function(we use multiple dispatch in Julia). This function takes four inputs: 


* `timestamp` (which is the data frame start and end dates)
* `concurrencyEvents` (which is data frame of concurrent events generated by the function concurrencyEvents)
* `returns` (which is data frame of returns)
*  `molecule` (the indices used when multithreading is applied). 

Similarly, in RiskLabAI's python library, the function `mpSampleWeightAbsoluteReturn` does the job.

<table style="width:80%">
<tr><th style="width:50%; text-align: center">Julia</th>
<th style="width:50%; text-align: center">Python</th></tr><tr>
<td style="border: 1px solid transparent">

```julia 
function sampleWeight(timestamp :pd.DataFrame, 
                      concurrencyEvents :pd.DataFrame,
                      returns :pd.Series,
                      molecule :np.array)
```
</td><td style="border: 1px solid transparent">

```python
def mpSampleWeightAbsoluteReturn(
      timestamp :pd.DataFrame,
      concurrencyEvents :pd.DataFrame,
      returns :pd.DataFrame,
      molecule :np.array
      ) -> None: 
```
</td></tr><tr><td colspan="2" style="text-align: center">
        View More: <a href="https://www.github.com/risklabai/RiskLabAI.jl">Julia</a> | <a href="https://www.github.com/risklabai/RiskLabAI.jl">Python</a>
</td></tr></table>
## It is important when the events happened!

Markets are adaptive systems (Lo [2017]). As markets evolve, older examples are less relevant than the newer ones. Consequently, we would typically like sample weights to decay as new observations arrive. Let $d[x] \geq 0, \forall x \in\left[0, \sum_{i=1}^{I} \bar{u}_{i}\right]$ be the time-decay factors that will multiply the sample weights derived in the previous section. The final weight has no decay, $d\left[\sum_{i=1}^{l} \bar{u}_{i}\right]=1$, and all other weights will be adjusted relative to that. Let $c \in(-1,1]$ be a user-defined parameter that determines the decay function as follows: For $c \in[0,1]$, then $d[1]=c$, with linear decay; for $c \in$ $(-1,0)$, then $d\left[-c \sum_{i=1}^{I} \bar{u}_{i}\right]=0$, with linear decay between $\left[-c \sum_{i=1}^{I} \bar{u}_{i}, \sum_{i=1}^{I} \bar{u}_{i}\right]$ and $d[x]=0 \forall x \leq-c \sum_{i=1}^{I} \bar{u}_{i}$. For a linear piecewise function $d=\max \{0, a+b x\}$, such requirements are met by the following boundary conditions:

1. $d=a+b \sum_{i=1}^{I} \bar{u}_{i}=1 \Rightarrow a=1-b \sum_{i=1}^{I} \bar{u}_{i}$.

2. Contingent on $\mathrm{c}$ :

(a) $d=a+b 0=c \Rightarrow b=(1-c)\left(\sum_{i=1}^{I} \bar{u}_{i}\right)^{-1}, \forall c \in[0,1]$

(b) $d=a-b c \sum_{i=1}^{l} \bar{u}_{i}=0 \Rightarrow b=\left[(c+1) \sum_{i=1}^{l} \bar{u}_{i}\right]^{-1}, \forall c \in(-1,0)$

Snippet $4.11$ implements this form of time-decay factors. Note that time is not meant to be chronological. In this implementation, decay takes place according to cumulative uniqueness, $x \in\left[0, \sum_{i=1}^{I} \bar{u}_{i}\right]$, because a chronological decay would reduce weights too fast in the presence of redundant observations.


In the RiskLabAI's Julia library, we changed weight based on time with  `timeDecay` function. This function takes two input: 
* `weight` (which is data frame that contain weights of events). 
* `clfLastW` (which show weight of oldest observation )
  
Similarly, in RiskLabAI's python library, the function TBD does the job.
<table style="width:80%">
<tr><th style="width:50%; text-align: center">Julia</th>
<th style="width:50%; text-align: center">Python</th></tr><tr>
<td style="border: 1px solid transparent"> 

```julia
function TimeDecay(
      weight; 
      clfLastW = 1.0
)::Nothing 
```
</td><td style="border: 1px solid transparent">

```python
def timeDecay(
      weight :pd.Series, 
      clfLastW = 1.0 :float
      ) -> None:
```
</td></tr><tr><td colspan="2" style="text-align: center">
        View More: <a href="https://www.github.com/risklabai/RiskLabAI.jl">Julia</a> | <a href="https://www.github.com/risklabai/RiskLabAI.jl">Python</a>
</td></tr></table>

It is worth discussing a few interesting cases:

- $c=1$ means that there is no time decay.

- $0<c<1$ means that weights decay linearly over time, but every observation still receives a strictly positive weight, regardless of how old. 

<figure><img src="Figs\timedecay.png" width="50%" alt="put alternative text here..."/>
<figcaption><span style="color:DimGray; font-weight:bold">Time decay</span></figcaption></figure>



FIGURE 4.3 Piecewise-linear time-decay factors

- $c=0$ means that weights converge linearly to zero, as they become older.

- $c<0$ means that the oldest portion $c T$ of the observations receive zero weight (i.e., they are erased from memory).

Figure $4.3$ shows the decayed weights, out $\left[\mathrm{w}^{\prime}\right] * \mathrm{df}$, after applying the decay factors for $c \in\{1, .75, .5,0,-.25,-.5\}$. Although not necessarily practical, the procedure allows the possibility of generating weights that increase as they get older, by setting $c>1$.