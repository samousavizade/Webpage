import {LazyPlot} from "@/components/article_page/plotly_figure";
import plotData from "./Standard Deviation of the bagged prediction (function of œÅ and N).json";
import { Typography } from "@mui/material";

# Ensemble Methods


## You have most likely utilized <span style={{color:"cyan"}}>ensemble approaches</span> to avoid overfitting!


Let us discuss what makes ensemble techniques useful and how to prevent frequent mistakes that lead to their abuse in finance.


## How can we break down <span style={{color:"cyan"}}>algorithmic errors</span>?


Three errors due to unrealistic assumptions are common in machine learning models. The model fails to detect critical feature-outcome relationships when the bias is significant, resulting in an underfit. Variance error is created by the training set's sensitivity to small movements in the market. When the variance is substantial, the model has overfitted the training set. Therefore even little changes in the training set samples might result in different predictions. When overfitting is present, the noise is calibrated as the signal instead of modeling the patterns in the training data.


Consider a collection of training observations and real-valued outcomes $\left\{(x_i, y_i)\right\}_{i=1}^n$. Assume there is a function $f(x)$ such that:


$$z

y=f(x)+\varepsilon ,

$$


where $\varepsilon$ is white noise with mean $0$ and standard deviation $\sigma_{\varepsilon}$. We would want to estimate the function $\hat{f}[x]$ that best matches $f(x)$, by minimizing the variance of the estimation error:


$$

\mathbb{E}\left[\left(y_{i}-\hat{f}\left[x_{i}\right]\right)^{2}\right].

$$


This mean-squared error can be broken down as follows:


$$

\mathbb{E}\left[\left(y_{i}-\hat{f}\left[x_{i}\right]\right)^{2}\right]=\left({\mathbb{E}\left[\hat{f}\left[x_{i}\right]-f\left[x_{i}\right]\right]}\right)^{2}+{\mathrm{Var}\left[\hat{f}\left[x_{i}\right]\right]}+{\sigma_{\varepsilon}^{2}}

$$


An ensemble approach is a method that combines a group of weak learners that use the same learning algorithm to build a stronger learner who outperforms all the base learners.


## How can model aggregation be used to <span style={{color:"cyan"}}>reduce variance</span>?


Bootstrap aggregation, often known as bagging, is an efficient method for lowering prediction variation. It works like this: To begin, create $N$ training datasets using random sampling with replacement. Second, for each training set, fit $N$ estimators. Because these estimators are trained independently, the models can be fit together. Third, the ensemble prediction is the simple average of the $N$ models' forecasts. For classification problems, the ratio of base estimators that vote for a specific outcome can form a pseudo-probability indicating how likely that label can be assigned to a test set sample. The bagging classifier may calculate the mean of the probabilities when the base estimator provides a probability forecast. For example, assume you have a series of Gaussian Mixture models giving you probabilities that a given sample $x_\text{new}$ can be assigned to any of your $K$ labels. If there is an ensemble of N GMMs, then we can calculate the mean of these probabilities across all the N base estimators.


## Let us show mathematically how the bagging method we just discussed <span style={{color:"cyan"}}>reduces variance</span>?


The fundamental advantage of bagging is that it minimizes forecast variance to address overfitting. The variance of the bagged prediction $\left(\varphi_{i}[c]\right)$ varies with the number of base estimators $(N)$, the average variance of a single estimator's prediction $(\bar{\sigma})$, and the average correlation between their predictions $(\bar{\rho})$:


$$

\begin{aligned}

\mathrm{Var}\left[\frac{1}{N} \sum_{i = 1}^{N} \varphi_{i}(c)\right]

&=\frac{1}{N ^ {2}} \sum_{i = 1}^{N}\left(\sum_{j = 1}^{N} \sigma_{i, j}\right)=\frac{1}{N ^ {2}} \sum_{i = 1}^{N}\left(\sigma_{i}^{2}+\sum_{j \neq i}^{N} \sigma_{i} \sigma_{j} \rho_{i, j}\right)

\\

&=\frac{1}{N ^ {2}} \sum_{i = 1}^{N}(\bar{\sigma}^{2}+{\sum_{j \neq i}^{N} \bar{\sigma}^{2} \bar{\rho}})=\frac{\bar{\sigma}^{2}+(N-1) \bar{\sigma}^{2} \bar{\rho}}{N} \\

&=\bar{\sigma}^{2}\left(\bar{\rho}+\frac{1 -\bar{\rho}}{N}\right),

\end{aligned}

$$


where $\sigma_{i, j}$ is the covariance of predictions by estimators $i, j$, which gives a formula for the average correlation $\bar{\rho}$:


$$

\bar{\rho}=\left(\bar{\sigma}^{2} N(N-1)\right)^{-1}\sum_{j \neq i}^{N} \sigma_{i} \sigma_{j} \rho_{i, j} .

$$


The preceding equation demonstrates that bagging is only helpful to the degree that $\bar{\rho}<$ $1;$ as


$$

\bar{\rho} \rightarrow 1 \Rightarrow \mathrm{Var}\left[\frac{1}{N} \sum_{i = 1}^{N} \varphi_{i}[c]\right]

\rightarrow \bar{\sigma}^{2}.

$$


One of the benefits of sequential bootstrapping is to generate samples that are as independent as possible, lowering $\bar{\rho}$ and the variance of bagging classifiers.


The standard deviation of the bagged prediction is plotted in this Figure as a function of $N \in[5,30], \bar{\rho} \in[0,1]$ and $\bar{\sigma}=1$.


<LazyPlot {...plotData} plotType={"Heatmap"} />


## How does the bagging classifier's voting mechanism <span style={{color:"cyan"}}>improves accuracy</span>?

Consider a bagging classifier that predicts $k$ classes based on a majority vote among $N$ independent classifiers with a $\{0,1\}$ outcome. A base classifier's accuracy is the probability $p$ of identifying a correct prediction as label 1. We will obtain $N p$ predictions labeled as 1 on average, with a famously known variance of $N p(1-p)$. When a class with the highest votes is observed, majority voting makes an accurate prediction. A sufficient condition is that the total of these labels is greater than $\frac{N}{2}$. A required condition is that $X>\frac{N}{k}$ happens with probability:

$$
\mathbb{P}\left[X>\frac{N}{k}\right]=1-\mathbb{P}\left[X \leq \frac{N}{k}\right]=1-\sum_{i=0}^{\lfloor N / k\rfloor}\left(\begin{array}{c}
N \\
i
\end{array}\right) p^{i}(1-p)^{N-i}
$$

The consequence is that given a sufficiently big $N$, say $N>p(p-1 / k)^{-2}$, we have:

$$
p>\frac{1}{k} \Rightarrow \mathbb{P}\left[X>\frac{N}{k}\right]>p
$$

and so the bagging classifier's accuracy surpasses the individual classifiers' average accuracy.

This is a compelling case for bagging any classifier in general when computing constraints allow it. Unlike boosting, however, bagging cannot enhance the accuracy of bad classifiers: Majority voting will still perform poorly if the individual learners are poor classifiers $\left(p \ll \frac{1}{k}\right.$) (although with lower variance). Figure $6.2$ depicts these facts. Bagging is more likely to be effective in lowering variance than in reducing bias since it is simpler to attain $\bar{\rho} \ll 1$ than $p>\frac{1}{k}$.


In the RiskLabAI's Julia library, the bagging classifier accuracy is calculated using the `baggingClassifierAccuracy` function. This function takes 3 inputs:
* `N` (the number of independent base classifiers),
* `p` (the accuracy of a classifier that labeling a prediction as 1 with the probability of p.),
* `k` (the number ofc classes).

Similarly, in RiskLabAI's python library, the function `bagging_classifier_accuracy` does the job.


<CH.Code>

    ```julia Julia
    function baggingClassifierAccuracy(
    N::Int,
    p::Float64,
    k::Int,
    )::Float64

    ```

    ```python Python
    def bagging_classifier_accuracy(
    N: int,
    p: float,
    k: int = 2,
    ) -> float:

    ```

</CH.Code>


<Typography variant={"h7"} textAlign={"center"}>
    View More: <a style={{color: "purple", fontWeight: "bold"}} href="https://www.github.com/risklabai/RiskLabAI.jl">Julia</a> | <a style={{color: "blue", fontWeight: "bold"}} href="https://www.github.com/risklabai/RiskLabAI.jl">Python</a>
</Typography>




The accuracy of the bagged prediction is plotted in this Figure:




--- until here ---


## What challenges the <span style={{color:"cyan"}}>dependency structure</span> in the observation set will bring to our bagging framework?


We know that financial observations cannot be presumed to be IIDs. Redundant observations can bring two drawbacks to bagging:


* First, replacement samples are more likely to be nearly similar, even if they do not share the same findings. This makes $\bar{\rho} \approx 1$ and therefore bagging will not reduce variance regardless of $N$.

* The second negative impact of observation redundancy is increased out-of-bag accuracy. This occurs because random sampling with replacement places samples in the training set that are highly similar to those from the bag. In this scenario, a suitable stratified k-fold cross-validation without shuffling before partitioning will yield a significantly lower validation-set accuracy than the one calculated out-of-bag.



