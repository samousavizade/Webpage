[//]: # (import Fund from "./bplotly";)

[//]: # ()
[//]: # (# Ensemble Methods)

[//]: # ()
[//]: # (## You have most likely utilized <span style={{color:"cyan"}}>ensemble approaches</span> to avoid overfitting!)

[//]: # ()
[//]: # (Let us discuss what makes ensemble techniques useful and how to prevent frequent mistakes that lead to their abuse in finance.)

[//]: # ()
[//]: # (## How can we break down <span style={{color:"cyan"}}>algorithmic errors</span>?)

[//]: # ()
[//]: # (Three errors due to unrealistic assumptions are common in machine learning models. The model fails to detect critical feature-outcome relationships when the bias is significant, resulting in an underfit. Variance error is created by the training set's sensitivity to small movements in the market. When the variance is substantial, the model has overfitted the training set. Therefore even little changes in the training set samples might result in different predictions. When overfitting is present, the noise is calibrated as the signal instead of modeling the patterns in the training data.)

[//]: # ()
[//]: # (Consider a collection of training observations and real-valued outcomes $\left\{&#40;x_i, y_i&#41;\right\}_{i=1}^n$. Assume there is a function $f&#40;x&#41;$ such that:)

[//]: # ()
[//]: # (<center>$y=f&#40;x&#41;+\varepsilon ,$</center>)

[//]: # ()
[//]: # (where $\varepsilon$ is white noise with mean $0$ and standard deviation $\sigma_{\varepsilon}$. We would want to estimate the function $\hat{f}[x]$ that best matches $f&#40;x&#41;$, by minimizing the variance of the estimation error:)

[//]: # ()
[//]: # (<center>)

[//]: # (  $\mathbb{E}\left[\left&#40;y_{i}-\hat{f}\left[x_{i}\right]\right&#41;^{2}\right].$)

[//]: # (</center>)

[//]: # ()
[//]: # (This mean-squared error can be broken down as follows:)

[//]: # ()
[//]: # (<center>)

[//]: # (    $)

[//]: # (    \mathbb{E}\left[\left&#40;y_{i}-\hat{f}\left[x_{i}\right]\right&#41;^{2}\right]=\left&#40;{\mathbb{E}\left[\hat{f}\left[x_{i}\right]-f\left[x_{i}\right]\right]}\right&#41;^{2}+{\mathrm{Var}\left[\hat{f}\left[x_{i}\right]\right]}+{\sigma_{\varepsilon}^{2}})

[//]: # (    $)

[//]: # (</center>)

[//]: # ()
[//]: # (An ensemble approach is a method that combines a group of weak learners that use the same learning algorithm to build a stronger learner who outperforms all the base learners.)

[//]: # ()
[//]: # (## How can model aggregation be used to <span style={{color:"cyan"}}>reduce variance</span>?)

[//]: # ()
[//]: # (Bootstrap aggregation, often known as bagging, is an efficient method for lowering prediction variation. It works like this: To begin, create $N$ training datasets using random sampling with replacement. Second, for each training set, fit $N$ estimators. Because these estimators are trained independently, the models can be fit together. Third, the ensemble prediction is the simple average of the $N$ models' forecasts. For classification problems, the ratio of base estimators that vote for a specific outcome can form a pseudo-probability indicating how likely that label can be assigned to a test set sample. The bagging classifier may calculate the mean of the probabilities when the base estimator provides a probability forecast. For example, assume you have a series of Gaussian Mixture models giving you probabilities that a given sample $x_\text{new}$ can be assigned to any of your $K$ labels. If there is an ensemble of N GMMs, then we can calculate the mean of these probabilities across all the N base estimators.)

[//]: # ()
[//]: # (## Let us show mathematically how the bagging method we just discussed <span style={{color:"cyan"}}>reduces variance</span>?)

[//]: # ()
[//]: # (The fundamental advantage of bagging is that it minimizes forecast variance to address overfitting. The variance of the bagged prediction $\left&#40;\varphi_{i}[c]\right&#41;$ varies with the number of base estimators $&#40;N&#41;$, the average variance of a single estimator's prediction $&#40;\bar{\sigma}&#41;$, and the average correlation between their predictions $&#40;\bar{\rho}&#41;$:)

[//]: # ()
[//]: # (<center>)

[//]: # (    $)

[//]: # (    \begin{aligned})

[//]: # (    \mathrm{Var}\left[\frac{1}{N} \sum_{i=1}^{N} \varphi_{i}&#40;c&#41;\right] &=\frac{1}{N^{2}} \sum_{i=1}^{N}\left&#40;\sum_{j=1}^{N} \sigma_{i, j}\right&#41;=\frac{1}{N^{2}} \sum_{i=1}^{N}\left&#40;\sigma_{i}^{2}+\sum_{j \neq i}^{N} \sigma_{i} \sigma_{j} \rho_{i, j}\right&#41; \\)

[//]: # (    &=\frac{1}{N^{2}} \sum_{i=1}^{N}&#40;\bar{\sigma}^{2}+{\sum_{j \neq i}^{N} \bar{\sigma}^{2} \bar{\rho}}&#41;=\frac{\bar{\sigma}^{2}+&#40;N-1&#41; \bar{\sigma}^{2} \bar{\rho}}{N} \\)

[//]: # (    &=\bar{\sigma}^{2}\left&#40;\bar{\rho}+\frac{1-\bar{\rho}}{N}\right&#41;,)

[//]: # (    \end{aligned})

[//]: # (    $)

[//]: # (</center>)

[//]: # ()
[//]: # (where $\sigma_{i, j}$ is the covariance of predictions by estimators $i, j$, which gives a formula for the average correlation $\bar{\rho}$:)

[//]: # ()
[//]: # (<center>)

[//]: # (    $)

[//]: # (    \bar{\rho}=\left&#40;\bar{\sigma}^{2} N&#40;N-1&#41;\right&#41;^{-1}\sum_{j \neq i}^{N} \sigma_{i} \sigma_{j} \rho_{i, j} .)

[//]: # (    $)

[//]: # (</center>)

[//]: # ()
[//]: # (The preceding equation demonstrates that bagging is only helpful to the degree that $\bar{\rho}<$ $1;$ as)

[//]: # ()
[//]: # (<center>)

[//]: # ($)

[//]: # (\bar{\rho} \rightarrow 1 \Rightarrow \mathrm{Var}\left[\frac{1}{N} \sum_{i=1}^{N} \varphi_{i}[c]\right] \rightarrow \bar{\sigma}^{2}.)

[//]: # ($)

[//]: # (</center>)

[//]: # ()
[//]: # (One of the benefits of sequential bootstrapping is to generate samples that are as independent as possible, lowering $\bar{\rho}$ and the variance of bagging classifiers.)

[//]: # ()
[//]: # (The standard deviation of the bagged prediction is plotted in this Figure as a function of $N \in[5,30], \bar{\rho} \in[0,1]$ and $\bar{\sigma}=1$.)

[//]: # ()
[//]: # (<Fund />)
